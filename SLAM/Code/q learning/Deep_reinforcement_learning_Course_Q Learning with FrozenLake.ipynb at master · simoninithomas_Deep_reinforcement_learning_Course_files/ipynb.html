<!DOCTYPE html>
<!-- saved from url=(0686)https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&nwo=simoninithomas%2FDeep_reinforcement_learning_Course&path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&repository_id=126685014&repository_type=Repository#4e33dfca-931f-4a20-bf58-b5752e60c1c1 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Render</title>
  <meta name="referrer" content="never">
    <script src="./ipynb-0af7578295a2e6aa289c70ec5144846e.js"></script><link rel="stylesheet" href="./ipynb-cdc8a2c29b9e77afb5ead41e895d2850.css">


</head>
<body class="" data-render-url="https://render.githubusercontent.com" data-github-hostname="github.com" style="">
  <div class="render-shell js-render-shell" data-document-nwo="simoninithomas/Deep_reinforcement_learning_Course" data-document-commit="1ee37cfc3130057f828f19b3cee6066d41c1eeb4" data-document-path="Q learning/FrozenLake/Q Learning with FrozenLake.ipynb" data-file="https://github-render.s3.amazonaws.com/prod/ce5dd948981fbddbd0ab49f75817c3f9-render.html?AWSAccessKeyId=AKIAJILR36AMCOMBK3MQ&amp;Signature=BXdrot6/F6si5eFCRjTwlOJ478A%3D&amp;Expires=1558202471" data-meta="https://github-render.s3.amazonaws.com/prod/ce5dd948981fbddbd0ab49f75817c3f9-meta.json?AWSAccessKeyId=AKIAJILR36AMCOMBK3MQ&amp;Signature=tiLDTRBhFg6sg9Ue1IvdeMJjamc%3D&amp;Expires=1558202471">
    

<div class="render-info">
  <div class="js-viewer-health render-health is-viewer-good">
    <span class="symbol">âŠ–</span>
    <span class="js-message message">Everything running smoothly!</span>
  </div>
</div>

<div id="notebook" class="js-html">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>Q* Learning with FrozenLake ğŸ•¹ï¸â›„<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Q*-Learning-with-FrozenLake-%F0%9F%95%B9%EF%B8%8F%E2%9B%84">Â¶</a>
</h1>
<p><br> 
In this Notebook, we'll implement an agent <b>that plays FrozenLake.</b>
<img src="./frozenlake.png" alt="Frozen Lake"></p>
<p>The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1>This is a notebook from <a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/">Deep Reinforcement Learning Course with Tensorflow</a><a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#This-is-a-notebook-from-Deep-Reinforcement-Learning-Course-with-Tensorflow">Â¶</a>
</h1>
<p><img src="./68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f6d61737465722f646f63732f6173736574732f696d672f44524c43253230456e7669726f6e6d656e74732e7" alt="Deep Reinforcement Course" data-canonical-src="https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png">
<br></p>
<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials ğŸ†• about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradientsâ€¦), and how to implement them with Tensorflow.**
<br><br>

ğŸ“œThe articles explain the architectures from the big picture to the mathematical details behind them.
<br>
ğŸ“¹ The videos explain how to build the agents with Tensorflow </p>
<br>
This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: DoomÂ© ğŸ‘¹, Space invaders ğŸ‘¾, Outrun, Sonic the HedgehogÂ©, Michael Jacksonâ€™s Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. 
<br><br>
<h2>ğŸ“š The complete <a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/">Syllabus HERE</a><a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#%F0%9F%93%9A-The-complete-Syllabus-HERE">Â¶</a>
</h2>
<h2>Any questions ğŸ‘¨â€ğŸ’»<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Any-questions-%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB">Â¶</a>
</h2>
<p> If you have any questions, feel free to ask me: </p>
<p> ğŸ“§: <a href="mailto:hello@simoninithomas.com">hello@simoninithomas.com</a>  </p>
<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>
<p> ğŸŒ : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>
<p> Twitter: <a href="https://twitter.com/ThomasSimonini">@ThomasSimonini</a> </p>
<p> Don't forget to <b> follow me on <a href="https://twitter.com/ThomasSimonini">twitter</a>, <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course">github</a> and <a href="https://medium.com/@thomassimonini">Medium</a> to be alerted of the new articles that I publish </b></p>
<h2>How to help  ğŸ™Œ<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#How-to-help--%F0%9F%99%8C">Â¶</a>
</h2>
<p>3 ways:</p>
<ul>
<li>
<strong>Clap our articles and like our videos a lot</strong>:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.</li>
<li>
<strong>Share and speak about our articles and videos</strong>: By sharing our articles and videos you help us to spread the word. </li>
<li>
<strong>Improve our notebooks</strong>: if you found a bug or <strong>a better implementation</strong> you can send a pull request.
<br>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Prerequisites ğŸ—ï¸<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Prerequisites-%F0%9F%8F%97%EF%B8%8F">Â¶</a>
</h2>
<p>Before diving on the notebook <strong>you need to understand</strong>:</p>
<ul>
<li>The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419">Article</a>
</li>
<li>Q-learning <a href="https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe">Article</a>
</li>
<li>In the <a href="https://www.youtube.com/watch?v=q2ZOEFAaaI0">video version</a>  we implemented a Q-learning agent that learns to play OpenAI Taxi-v2 ğŸš• with Numpy.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">'&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/q2ZOEFAaaI0?showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;'</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[1]:</div>

<div class="output_html rendered_html output_subarea output_execute_result">

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Step 0: Import the dependencies ğŸ“š<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Step-0:-Import-the-dependencies-%F0%9F%93%9A">Â¶</a>
</h2>
<p>We use 3 libraries:</p>
<ul>
<li>
<code>Numpy</code> for our Qtable</li>
<li>
<code>OpenAI Gym</code> for our FrozenLake Environment</li>
<li>
<code>Random</code> to generate random numbers</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">random</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Step 1: Create the environment ğŸ®<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Step-1:-Create-the-environment-%F0%9F%8E%AE">Â¶</a>
</h2>
<ul>
<li>Here we'll create the FrozenLake environment. </li>
<li>OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>
</li>
<li>In our case we choose to use Frozen Lake.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">"FrozenLake-v0"</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Step 2: Create the Q-table and initialize it ğŸ—„ï¸<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Step-2:-Create-the-Q-table-and-initialize-it-%F0%9F%97%84%EF%B8%8F">Â¶</a>
</h2>
<ul>
<li>Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size</li>
<li>OpenAI Gym provides us a way to do that: <code>env.action_space.n</code> and <code>env.observation_space.n</code>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">action_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">state_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">qtable</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qtable</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]
 [0. 0. 0. 0.]]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Step 3: Create the hyperparameters âš™ï¸<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Step-3:-Create-the-hyperparameters-%E2%9A%99%EF%B8%8F">Â¶</a>
</h2>
<ul>
<li>Here, we'll specify the hyperparameters</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">total_episodes</span> <span class="o">=</span> <span class="mi">15000</span>        <span class="c1"># Total episodes</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.8</span>           <span class="c1"># Learning rate</span>
<span class="n">max_steps</span> <span class="o">=</span> <span class="mi">99</span>                <span class="c1"># Max steps per episode</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span>                  <span class="c1"># Discounting rate</span>

<span class="c1"># Exploration parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>                 <span class="c1"># Exploration rate</span>
<span class="n">max_epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>             <span class="c1"># Exploration probability at start</span>
<span class="n">min_epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>            <span class="c1"># Minimum exploration probability </span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.005</span>             <span class="c1"># Exponential decay rate for exploration prob</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Step 4: The Q learning algorithm ğŸ§ <a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Step-4:-The-Q-learning-algorithm-%F0%9F%A7%A0">Â¶</a>
</h2>
<ul>
<li>Now we implement the Q learning algorithm:
<img src="./qtable_algo.png" alt="Q algo">
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># List of rewards</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 2 For life or until learning is stopped</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_episodes</span><span class="p">):</span>
    <span class="c1"># Reset the environment</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">total_rewards</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
        <span class="c1"># 3. Choose an action a in the current world state (s)</span>
        <span class="c1">## First we randomize a number</span>
        <span class="n">exp_exp_tradeoff</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="c1">## If this number &gt; greater than epsilon --&gt; exploitation (taking the biggest Q value for this state)</span>
        <span class="k">if</span> <span class="n">exp_exp_tradeoff</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span>

        <span class="c1"># Else doing a random choice --&gt; exploration</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="c1"># Take the action (a) and observe the outcome state(s') and reward (r)</span>
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]</span>
        <span class="c1"># qtable[new_state,:] : all the actions we can take from new state</span>
        <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">new_state</span><span class="p">,</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>
        
        <span class="n">total_rewards</span> <span class="o">+=</span> <span class="n">reward</span>
        
        <span class="c1"># Our new state is state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
        
        <span class="c1"># If done (if we're dead) : finish episode</span>
        <span class="k">if</span> <span class="n">done</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span> 
            <span class="k">break</span>
        
    <span class="c1"># Reduce epsilon (because we need less and less exploration)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">min_epsilon</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_epsilon</span> <span class="o">-</span> <span class="n">min_epsilon</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">decay_rate</span><span class="o">*</span><span class="n">episode</span><span class="p">)</span> 
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">"Score over time: "</span> <span class="o">+</span>  <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">/</span><span class="n">total_episodes</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">qtable</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Score over time: 0.4755333333333333
[[3.09661199e-01 4.20986767e-02 4.09817720e-02 4.33154671e-02]
 [3.04309088e-03 1.77615720e-02 1.75027968e-04 4.48805036e-02]
 [1.17515610e-02 3.49659785e-03 1.25602764e-02 1.45895688e-02]
 [5.30730075e-03 2.00738408e-03 2.10082319e-03 1.03044803e-02]
 [3.74544071e-01 1.14433376e-02 4.25301395e-02 8.92078716e-03]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [2.45730220e-03 5.11951837e-05 2.32423145e-06 4.80236578e-07]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [1.15951273e-01 2.26517591e-02 2.95426375e-03 4.22247574e-01]
 [2.73740942e-03 2.56680897e-01 5.08957170e-02 5.09211745e-02]
 [7.61741394e-03 7.11600600e-01 3.66761331e-03 1.12599083e-02]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
 [3.78622225e-02 2.89343711e-02 4.23222346e-01 5.43340302e-02]
 [1.34016966e-01 1.90320465e-01 1.39202525e-01 8.99555845e-01]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Step 5: Use our Q-table to play FrozenLake ! ğŸ‘¾<a class="anchor-link" href="https://render.githubusercontent.com/view/ipynb?commit=1ee37cfc3130057f828f19b3cee6066d41c1eeb4&amp;enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f73696d6f6e696e6974686f6d61732f446565705f7265696e666f7263656d656e745f6c6561726e696e675f436f757273652f316565333763666333313330303537663832386631396233636565363036366434316331656562342f512532306c6561726e696e672f46726f7a656e4c616b652f512532304c6561726e696e672532307769746825323046726f7a656e4c616b652e6970796e62&amp;nwo=simoninithomas%2FDeep_reinforcement_learning_Course&amp;path=Q+learning%2FFrozenLake%2FQ+Learning+with+FrozenLake.ipynb&amp;repository_id=126685014&amp;repository_type=Repository#Step-5:-Use-our-Q-table-to-play-FrozenLake-!-%F0%9F%91%BE">Â¶</a>
</h2>
<ul>
<li>After 10 000 episodes, our Q-table can be used as a "cheatsheet" to play FrozenLake"</li>
<li>By running this cell you can see our agent playing FrozenLake.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"****************************************************"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"EPISODE "</span><span class="p">,</span> <span class="n">episode</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
        
        <span class="c1"># Take the action (index) that have the maximum expected future reward given that state</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">qtable</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span>
        
        <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)</span>
            <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
            
            <span class="c1"># We print the number of step it took.</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Number of steps"</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
 

</div>

  </div>

  



</body></html>